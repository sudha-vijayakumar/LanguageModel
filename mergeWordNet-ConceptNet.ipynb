{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25e9aed-5323-4d89-958a-7d427d390210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  449M  100  449M    0     0  14.1M      0  0:00:31  0:00:31 --:--:-- 21.6M\n"
     ]
    }
   ],
   "source": [
    "!curl ${1:-https://s3.amazonaws.com/conceptnet/downloads/2018/edges/conceptnet-assertions-5.6.0.csv.gz} --output conceptnet-assertions.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dce433-e28a-42b1-8216-dacd05641979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:\n",
    "#https://github.com/tomkdickinson/wordnet_conceptnet_neo4j\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import csv\n",
    "import json\n",
    "import logging as log\n",
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "log.basicConfig(level=log.INFO)\n",
    "\n",
    "\n",
    "class WordNode:\n",
    "    def __init__(self, id, name, pos, is_lemma=False, is_concept=False, concept_uri=None):\n",
    "        self._id = id\n",
    "        self._name = name\n",
    "        self._pos = pos\n",
    "        self._concept_uri = concept_uri\n",
    "        self._is_lemma = is_lemma\n",
    "        self._is_concept = is_concept\n",
    "        if not is_lemma and not is_concept:\n",
    "            raise Exception('WordNode must be at least either a lemma or a concept')\n",
    "\n",
    "    @staticmethod\n",
    "    def get_header():\n",
    "        return ['id:ID', 'name', 'pos', 'conceptUri', ':LABEL']\n",
    "\n",
    "    def get_row(self):\n",
    "        labels = []\n",
    "        if self._is_lemma:\n",
    "            labels.append('Lemma')\n",
    "        if self._is_concept:\n",
    "            labels.append('Concept')\n",
    "        return [self._id, self._name, self._pos, self._concept_uri, ';'.join(labels)]\n",
    "\n",
    "    @property\n",
    "    def get_id(self):\n",
    "        return self._id\n",
    "\n",
    "    def set_is_concept(self):\n",
    "        self._is_concept = True\n",
    "\n",
    "    @property\n",
    "    def get_concept_uri(self):\n",
    "        return self._concept_uri\n",
    "\n",
    "    def set_concept_uri(self, concept_uri):\n",
    "        self._concept_uri = concept_uri\n",
    "\n",
    "\n",
    "class SynsetNode:\n",
    "    def __init__(self, id, pos, definition):\n",
    "        self._label = 'Synset'\n",
    "        self._id = id\n",
    "        self._pos = pos\n",
    "        self._definition = definition\n",
    "\n",
    "    @property\n",
    "    def get_id(self):\n",
    "        return self._id\n",
    "\n",
    "    @property\n",
    "    def get_pos(self):\n",
    "        return self._pos\n",
    "\n",
    "    @property\n",
    "    def get_definition(self):\n",
    "        return self._definition\n",
    "\n",
    "    @property\n",
    "    def get_label(self):\n",
    "        return self._label\n",
    "\n",
    "\n",
    "class Exporter:\n",
    "    \"\"\"\n",
    "    This class exports WordNet into a csv file for ingestion into Neo4J\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_folder, concept_location=None, language_filter=None):\n",
    "        self.dataset_folder = dataset_folder\n",
    "        os.makedirs(self.dataset_folder, exist_ok=True)\n",
    "        self.relationships = []\n",
    "        self.relationship_index = {}\n",
    "        self.words = []\n",
    "        self.synsets = []\n",
    "        self.lemma_map = {}\n",
    "        self.concept_location = concept_location\n",
    "        self.language_filter = language_filter\n",
    "\n",
    "    def export(self):\n",
    "        self.extract_wordnet()\n",
    "        if self.concept_location is not None:\n",
    "            self.extract_conceptnet()\n",
    "        self.write_results()\n",
    "\n",
    "    def extract_wordnet(self):\n",
    "        \"\"\"\n",
    "        Extracts wordnet\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        log.info('Extracting WordNet')\n",
    "        all_synsets = list(nltk.corpus.wordnet.all_synsets())\n",
    "        last_progress = 0\n",
    "        for i, synset in enumerate(all_synsets):\n",
    "            self.synsets.append(SynsetNode(synset.name(), synset.pos(), synset.definition()))\n",
    "            self.extract_relationships(synset)\n",
    "            self.extract_lemmas(synset)\n",
    "            progress = math.floor((i / len(all_synsets) * 100))\n",
    "            if progress > last_progress:\n",
    "                last_progress = progress\n",
    "                log.info(\"%f%% Synsets extracted\" % progress)\n",
    "\n",
    "    def extract_conceptnet(self):\n",
    "        log.info('Extracting ConceptNet')\n",
    "        last_progress = 0\n",
    "        with gzip.open(self.concept_location, 'rt') as f:\n",
    "            lines = csv.reader(f, delimiter='\\t')\n",
    "            i = 0\n",
    "            for line in lines:\n",
    "                start = self.extract_concept(line[2])\n",
    "                end = self.extract_concept(line[3])\n",
    "                if start is not None and end is not None:\n",
    "                    self.add_concept_node(start)\n",
    "                    self.add_concept_node(end)\n",
    "                    dataset, weight = self.extract_edge_details(line[4])\n",
    "                    self.add_relationship(start.get_id, end.get_id, re.sub('/r/', '', line[1]), dataset, weight)\n",
    "\n",
    "                i += 1\n",
    "                progress_round = 10000\n",
    "                progress = math.floor(i / progress_round) * progress_round\n",
    "                if progress > last_progress:\n",
    "                    last_progress = progress\n",
    "                    log.info('Extracted %i concept assertions' % progress)\n",
    "\n",
    "    def extract_edge_details(self, edge_string):\n",
    "        try:\n",
    "            dataset = None\n",
    "            weight = None\n",
    "            edge = json.loads(edge_string)\n",
    "            if 'dataset' in edge:\n",
    "                dataset = edge['dataset']\n",
    "            if 'weight' in edge:\n",
    "                try:\n",
    "                    weight = float(edge['weight'])\n",
    "                except Exception as e:\n",
    "                    log.error(e)\n",
    "            return dataset, weight\n",
    "        except Exception as e:\n",
    "            log.error(e)\n",
    "            return None, None\n",
    "\n",
    "    def add_concept_node(self, concept):\n",
    "        if concept is not None:\n",
    "            if concept.get_id in self.lemma_map:\n",
    "                self.lemma_map[concept.get_id.lower()].set_is_concept()\n",
    "            else:\n",
    "                self.lemma_map[concept.get_id.lower()] = concept\n",
    "            self.lemma_map[concept.get_id.lower()].set_concept_uri(concept.get_concept_uri)\n",
    "\n",
    "    def extract_concept(self, concept_uri):\n",
    "        concept_parts = concept_uri[1:].split('/')\n",
    "        if self.language_filter is None or concept_parts[1] == self.language_filter:\n",
    "            if len(concept_parts) > 3:\n",
    "                return WordNode(id='%s.%s' % (concept_parts[2], concept_parts[3]), pos=concept_parts[3],\n",
    "                                name=concept_parts[2], concept_uri=concept_uri, is_concept=True)\n",
    "            else:\n",
    "                return WordNode(id=concept_parts[2], pos=None, name=concept_parts[2], concept_uri=concept_uri,\n",
    "                                is_concept=True)\n",
    "        return None\n",
    "\n",
    "    def extract_relationships(self, synset):\n",
    "        # Hyponyms - Child\n",
    "        for related_node in synset.hyponyms():\n",
    "            self.add_relationship(related_node.name(), synset.name(), 'IsA', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # Hypernyms - Parent\n",
    "        for related_node in synset.hypernyms():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'IsA', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # Holonyms\n",
    "\n",
    "        # Member Holonyms\n",
    "        for related_node in synset.member_holonyms():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'PartOf', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # substance_holonyms\n",
    "        for related_node in synset.substance_holonyms():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'PartOf', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # part_holonyms\n",
    "        for related_node in synset.part_holonyms():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'PartOf', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # Meronyms Child\n",
    "\n",
    "        # Member meronyms\n",
    "        for related_node in synset.member_meronyms():\n",
    "            self.add_relationship(related_node.name(), synset.name(), 'PartOf', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # substance_meronyms\n",
    "        for related_node in synset.substance_meronyms():\n",
    "            self.add_relationship(related_node.name(), synset.name(), 'PartOf', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # part_meronyms\n",
    "        for related_node in synset.part_meronyms():\n",
    "            self.add_relationship(related_node.name(), synset.name(), 'PartOf', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # Domains\n",
    "\n",
    "        # topic_domains\n",
    "        for related_node in synset.topic_domains():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'Domain', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # region_domains\n",
    "        for related_node in synset.region_domains():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'Domain', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # usage_domains\n",
    "        for related_node in synset.usage_domains():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'Domain', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "        # attributes\n",
    "        for related_node in synset.attributes():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'Attribute', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "        # entailments\n",
    "        for related_node in synset.entailments():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'Entailment', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "        # causes\n",
    "        for related_node in synset.causes():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'Cause', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "        # also_sees\n",
    "        for related_node in synset.also_sees():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'AlsoSee', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "        # verb_groups\n",
    "        for related_node in synset.verb_groups():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'VerbGroup', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "        # similar_tos\n",
    "        for related_node in synset.similar_tos():\n",
    "            self.add_relationship(synset.name(), related_node.name(), 'SimilarTo', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "    def extract_lemmas(self, synset):\n",
    "        for lemma in synset.lemmas():\n",
    "            id = ('%s.%s' % (lemma.name().lower(), synset.pos())).lower()\n",
    "            if id not in self.lemma_map:\n",
    "                self.lemma_map[id] = WordNode(id, lemma.name().lower(), synset.pos(), is_lemma=True)\n",
    "            self.add_relationship(id, synset.name(), 'InSynset', weight=2, dataset=\"/d/wordnet/3.1\")\n",
    "\n",
    "    def index_relationship(self, start, end, rel_type):\n",
    "        self.relationship_index.setdefault(start, {})\n",
    "        self.relationship_index[start].setdefault(end, [])\n",
    "        self.relationship_index[start][end].append(rel_type)\n",
    "\n",
    "    def add_relationship(self, start, end, rel_type, dataset=None, weight=None):\n",
    "        \"\"\"\n",
    "        Checks and adds only bi-directional relationships\n",
    "        :param start:\n",
    "        :param end:\n",
    "        :param rel_type:\n",
    "        :param dataset:\n",
    "        :param weight:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if (start in self.relationship_index and end in self.relationship_index[start] and rel_type in\n",
    "            self.relationship_index[start][end]) or \\\n",
    "                (end in self.relationship_index and start in self.relationship_index[end] and rel_type in\n",
    "                 self.relationship_index[end][start]):\n",
    "            pass\n",
    "        else:\n",
    "            self.index_relationship(start, end, rel_type)\n",
    "            self.index_relationship(end, start, rel_type)\n",
    "            self.relationships.append([start, end, dataset, weight, rel_type])\n",
    "\n",
    "    def write_results(self):\n",
    "        log.info(\"Writing synsets\")\n",
    "        with open('%s/synsets.csv' % self.dataset_folder, 'w', encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['id:ID', 'pos:string', 'definition:string', ':LABEL'])\n",
    "            for synset in self.synsets:\n",
    "                writer.writerow([synset.get_id, synset.get_pos, synset.get_definition, synset.get_label])\n",
    "\n",
    "        log.info('Writing Relationships')\n",
    "        with open('%s/relationships.csv' % self.dataset_folder, 'w', encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([':START_ID', ':END_ID', 'dataset:string', 'weight:double', ':TYPE'])\n",
    "            for relationship in self.relationships:\n",
    "                writer.writerow(relationship)\n",
    "\n",
    "        log.info('Writing Words')\n",
    "        with open('%s/words.csv' % self.dataset_folder, 'w', encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(WordNode.get_header())\n",
    "            for id in self.lemma_map:\n",
    "                writer.writerow(self.lemma_map[id].get_row())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nltk.download('wordnet')\n",
    "    Exporter('csv_imports', 'conceptnet-assertions.csv.gz', language_filter='en').export()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
